{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88423af-f89f-45fc-b532-2d54190343d5",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c5591-7c5e-4856-85b8-47432bfe6942",
   "metadata": {},
   "source": [
    "# 1. What is a parameter? \n",
    "- A parameter refers to a internal variables of a model that are leanred from the trained data.These parameters define how the model makes predictions.\n",
    "- During training, the algorithm adjusts these parameters to minimize the error.\n",
    "- Once learned, parameters remain fixed when the model is used for making predictions.\n",
    "- Good parameters values is equal to better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fad3db-0db7-469b-baef-5ed4a83a4a40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. What is correlation?,What does negative correlation mean?\n",
    "- Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
    "- It is expressed with a value called the correlation coefficient(r), which ranges from -1 to +1.\n",
    "- In machine learning, correlation helps to identify how features are related to each other or to the target variable.\n",
    "- Types of correlation:\n",
    "1. Positive correlation (r > 0): When one variable increases, the other also increases.Example: Height and weight.\n",
    "2. Negative correlation (r < 0): When one variable increases,the other decreases. Example: Speed of a vehicle and travel time.\n",
    "3. Zero correlation ( r = 0): No clear relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c12d64-9d04-4082-b700-69adec75195e",
   "metadata": {},
   "source": [
    "# 3. Define Machine Learning. What are the main components in Machine Learning? \n",
    "- Machine learning is a branch of artificial intelligence  that enables systems to learn patterns from data and make predicitons or decisions without being explicitly programmed.\n",
    "- ML models train on historical data, identify hidden patterns, and use them to predict outcomes on new data.\n",
    "- example: predicting house prices, spam email detection, recommendation systems.\n",
    "The main components of Machine Learning are:\n",
    "1. Data\n",
    "\n",
    "- The foundation of ML.\n",
    "\n",
    "- Can be structured (tables), semi-structured (JSON, XML), or unstructured (images, text, videos).\n",
    "\n",
    "2. Features\n",
    "\n",
    "- Input variables (independent variables) used by the model.\n",
    "\n",
    "- Example: In predicting house price, features could be size, location, number of rooms.\n",
    "\n",
    "3. Model\n",
    "\n",
    "- The mathematical or algorithmic structure that learns from data.\n",
    "\n",
    "- Example: Linear Regression, Decision Tree, Neural Network.\n",
    "\n",
    "4. Parameters\n",
    "\n",
    "- Internal variables that the model learns from training data (e.g., weights and biases in regression or neural networks).\n",
    "\n",
    "5. Hyperparameters\n",
    "\n",
    "- Configurations set before training, not learned from data.\n",
    "\n",
    "- Example: Learning rate, number of trees in Random Forest, number of hidden layers in Neural Networks.\n",
    "\n",
    "6. Training\n",
    "\n",
    "- The process of feeding data into the model and adjusting parameters to minimize error.\n",
    "\n",
    "7. Evaluation\n",
    "\n",
    "- Checking model performance using metrics such as accuracy, precision, recall, RMSE, etc.\n",
    "\n",
    "8. Prediction\n",
    "\n",
    "- Using the trained model to make decisions or forecasts on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a52f0-6bef-42ce-85c2-4f1007526e0c",
   "metadata": {},
   "source": [
    "# 4. How does loss value help in determining whether the model is good or not? \n",
    "- In ML , the loss value is numerical measure of how far the model's predictions are from the actual target values. It is calculated using a loss function such as Mean Squared Error(MSE) for regression or Cross-Entropy Loss for classification.\n",
    "- A low loss value means the model's predictions are close to the true outputs, which generally indicates good performance.\n",
    "- A high loss value means the predictions deviate significantly from the true outputs.\n",
    "- The loss value helps in the following ways:\n",
    "- Model training progress - During training, we expect the loss value to decrease as the model learns.\n",
    "- Comparing models - Models with lower loss on validation/test data are usually better.\n",
    "- Detecting overfitting/underfitting-\n",
    "- If training loss is very low but validation loss is high, the model is overfitting.\n",
    "- If both training and validation loss are high, the model is underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6ee69-7aad-4f7b-bd4d-ca43246a2329",
   "metadata": {},
   "source": [
    "# 5. What are continuous and categorical variables? \n",
    "- In ML and Statistics, variables are the features or attributes used to describe data.They can be broadly classified into continuous variables and categorical variables.\n",
    "1. Continuous Variables\n",
    "- These are numerical variables that can take infinite possible values within a given range.\n",
    "- They are measurable and not restricted to fixed categories.\n",
    "\n",
    "- Examples:\n",
    "\n",
    "Height (e.g., 170.5 cm, 172.2 cm)\n",
    "\n",
    "Weight (e.g., 65.3 kg, 66.7 kg)\n",
    "\n",
    "Temperature (e.g., 36.6°C, 37.1°C)\n",
    "\n",
    "They can be further divided into:\n",
    "\n",
    "Interval variables (no true zero, e.g., temperature in °C)\n",
    "\n",
    "Ratio variables (have a true zero, e.g., weight, age, income).\n",
    "\n",
    "2. Categorical Variables \n",
    "- These are variables that represent distinct categories or groups.\n",
    "- They are qualitative in nature and not directly measurable.\n",
    "- Examples:\n",
    "\n",
    "Gender (Male, Female, Other)\n",
    "\n",
    "Blood Group (A, B, AB, O)\n",
    "\n",
    "Marital Status (Single, Married, Divorced)\n",
    "\n",
    "They can be further divided into:\n",
    "\n",
    "Nominal variables (no natural order, e.g., color: Red, Blue, Green)\n",
    "\n",
    "Ordinal variables (have a natural order, e.g., education level: High School < Graduate < Postgraduate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3a446-386e-428c-8eb1-a81fa97d9be1",
   "metadata": {},
   "source": [
    "# 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "- In Machine Learning, most algorithms work with numerical data, not with text-based categories. Therefore, categorical variables need to be converted into a numerical format before training the model. This process is called encoding categorical variables.\n",
    "1. Label Encoding\n",
    "\n",
    "- Assigns a unique number to each category.\n",
    "\n",
    "- Example: Color = [Red, Blue, Green] → [0, 1, 2]\n",
    "\n",
    "- Suitable for ordinal data (where order matters).\n",
    "\n",
    "- Limitation: May introduce false order for nominal data.\n",
    "\n",
    "2. One-Hot Encoding\n",
    "\n",
    "- Creates binary columns for each category.\n",
    "\n",
    "- Example: Color = [Red, Blue, Green] → [1,0,0], [0,1,0], [0,0,1]\n",
    "\n",
    "- Suitable for nominal data (no order).\n",
    "\n",
    "- Limitation: Can cause “curse of dimensionality” when categories are too many.\n",
    "\n",
    "3. Ordinal Encoding\n",
    "\n",
    "- Assigns integers to categories based on their order/rank.\n",
    "\n",
    "- Example: Education = [High School, Graduate, Postgraduate] → [1, 2, 3]\n",
    "\n",
    "- Suitable only for ordinal categorical variables.\n",
    "\n",
    "4. Frequency / Count Encoding\n",
    "\n",
    "- Replaces a category with its frequency or count in the dataset.\n",
    "\n",
    "- Example: If City = [Delhi, Mumbai, Delhi, Pune] → [2, 1, 2, 1].\n",
    "\n",
    "- Useful when categories are many and data is imbalanced.\n",
    "\n",
    "5. Target Encoding (Mean Encoding)\n",
    "\n",
    "- Replace each category with the mean of target variable for that category.\n",
    "\n",
    "- Example (classification problem): If City=Delhi has average target value 0.8, then replace Delhi with 0.8.\n",
    "\n",
    "- Powerful but prone to data leakage, so used with care.\n",
    "\n",
    "6. Binary Encoding\n",
    "\n",
    "- Converts categories into binary numbers and then splits into separate columns.\n",
    "\n",
    "- Example: Category 1, 2, 3, 4 → 01, 10, 11, 100.\n",
    "\n",
    "- More compact than one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d49d60-6a9b-4c4d-90f4-e7defcf7e3bd",
   "metadata": {},
   "source": [
    "# 7. What do you mean by training and testing a dataset? \n",
    "- In Machine Learning, the available dataset is usually divided into two main parts: Training dataset and Testing dataset. This separation helps in building a model and then evaluating its performance.\n",
    "1. Training Dataset\n",
    "\n",
    "The training dataset is the portion of data used to teach the model.\n",
    "\n",
    "The model learns patterns, relationships, and weights/parameters from this data.\n",
    "\n",
    "Example: If we are predicting house prices, the training dataset contains historical records of house features (size, location, rooms) along with their actual prices.\n",
    "\n",
    "Purpose: To allow the model to learn how input features map to the target variable.\n",
    "\n",
    "2. Testing Dataset\n",
    "\n",
    "The testing dataset is a separate portion of data used to evaluate the trained model.\n",
    "\n",
    "The model makes predictions on this unseen data, and the predictions are compared with the actual values.\n",
    "\n",
    "Example: After training, we give the model new house features (not seen during training) and check how well it predicts the prices.\n",
    "\n",
    "Purpose: To measure how well the model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80363aa-e477-4446-afa8-8a79d1fd8c41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8. What is sklearn.preprocessing? \n",
    "- sklearn.preprocessing is a module in Scikit-Learn (sklearn) that provides tools and functions to prepare raw data for Machine Learning models.Since most ML algorithms work better when input data is in proper scale or encoded form, preprocessing ensures that features are normalized,standardized or encoded before training.\n",
    "- Key Functions in sklearn.preprocessing\n",
    "\n",
    "1. Scaling and Normalization\n",
    "\n",
    "- StandardScaler → Standardizes features (mean = 0, variance = 1).\n",
    "\n",
    "- MinMaxScaler → Scales features to a given range, usually [0,1].\n",
    "\n",
    "- Normalizer → Scales each sample individually to unit norm (useful in text classification or clustering).\n",
    "\n",
    "2. Encoding Categorical Data\n",
    "\n",
    "- LabelEncoder → Converts categorical labels (e.g., \"Male\", \"Female\") into numbers.\n",
    "\n",
    "- OneHotEncoder → Converts categorical variables into binary columns.\n",
    "\n",
    "- OrdinalEncoder → Encodes categorical features as integers, respecting order.\n",
    "\n",
    "3. Generating Polynomial Features\n",
    "- PolynomialFeatures → Creates new features by combining existing ones (useful in polynomial regression).\n",
    "\n",
    "4. Imputation (Handling Missing Data)\n",
    "\n",
    "- SimpleImputer → Fills missing values with mean, median, mode, or a constant.\n",
    "\n",
    "5. Discretization (Converting Continuous → Categorical)\n",
    "- KBinsDiscretizer → Divides continuous values into bins (e.g., Age groups)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407f6a5-dd84-4820-ac7d-d8e4a9eeb94b",
   "metadata": {},
   "source": [
    "# 9. What is a Test set? \n",
    "- A Test Set is a separate portion of the dataset that is not used during training but is kept aside to evaluate the performance of the trained model. It contains input features (X) along with the actual target values (y), which are used to compare the model’s predictions.\n",
    "- Key Points about Test Set:\n",
    "\n",
    "1. Purpose:\n",
    "\n",
    "- To check how well the model generalizes to unseen data.\n",
    "\n",
    "- To get a realistic estimate of model performance.\n",
    "\n",
    "2. Data Splitting:\n",
    "\n",
    "- Usually, the dataset is split into:\n",
    "\n",
    "- Training Set (e.g., 70–80%) → Used to train the model.\n",
    "\n",
    "- Test Set (e.g., 20–30%) → Used to test the model.\n",
    "\n",
    "3. Evaluation metrics on Test Set.\n",
    "- Classification: Accuracy, Precision, Recall, F1-score.\n",
    "- Regression: Mean Squared Error (MSE), Mean Absolute Error (MAE), R² score.\n",
    "\n",
    "4. Important Rule:\n",
    "\n",
    "The test set should never be used in training; otherwise, it causes data leakage and gives an unrealistic performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f174615-19aa-4121-815f-78a68c77e4a8",
   "metadata": {},
   "source": [
    "# 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
    "- In Python, we commonly use the function train_test_split from the sklearn.model_selection module to divide data into training and testing sets.\n",
    "\n",
    "Training set: Used to fit (train) the model.\n",
    "\n",
    "Test set: Used to evaluate the performance of the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687fa765-9be7-43b9-9340-243f320175e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Test Set: 0.0\n"
     ]
    }
   ],
   "source": [
    "# example of training and testing model \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5], [6]]\n",
    "y = [2, 4, 6, 8, 10, 12]\n",
    "\n",
    "# Split data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MSE on Test Set:\", mean_squared_error(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7415b12-1666-487a-895e-15118f18d542",
   "metadata": {},
   "source": [
    "- Approaching an ML problem involves a systematic set of steps:\n",
    "Collect data → Gather dataset from source.\n",
    "\n",
    "Prepare data → Clean missing values, encode categories, scale numbers.\n",
    "\n",
    "Split data → Training set to train, Test set to evaluate.\n",
    "\n",
    "Choose a model → Select suitable algorithm.\n",
    "\n",
    "Train the model → Fit it on training data.\n",
    "\n",
    "Evaluate the model → Use metrics (Accuracy, MSE, etc.) on test data.\n",
    "\n",
    "Improve & tune → Adjust parameters or try better models.\n",
    "\n",
    "Deploy & monitor → Put model in real use and track performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5db7e6-64ae-4667-89ba-34074e7487a8",
   "metadata": {},
   "source": [
    "# 11. Why do we have to perform EDA before fitting a model to the data? \n",
    "- Exploratory Data analysis  is the process of analyzing and visualizing a dataset before applying Machine Learning models. It helps us understand the structure, quality and patterns in the data.Performing EDA is essential because it ensures the model is trained on clean, meaningful and well-understood data.\n",
    "- Reasons to Perform EDA Before Model Fitting\n",
    "\n",
    "1. Understand the Data\n",
    "\n",
    "- Identify the type of variables (continuous, categorical).\n",
    "\n",
    "- Understand distributions, ranges, and relationships.\n",
    "\n",
    "2. Detect Missing Values & Outliers\n",
    "\n",
    "- Missing or extreme values can mislead the model.\n",
    "\n",
    "- EDA helps decide whether to impute, remove, or transform them.\n",
    "\n",
    "3. Identify Data Imbalance\n",
    "\n",
    "- In classification, one class may dominate (e.g., 90% positive, 10% negative).\n",
    "\n",
    "- EDA helps spot imbalance so techniques like SMOTE, resampling can be applied.\n",
    "\n",
    "4. Feature Relationships\n",
    "\n",
    "- Correlation analysis reveals which features are important or redundant.\n",
    "\n",
    "- Prevents multicollinearity in models.\n",
    "\n",
    "5. Choose Correct Preprocessing\n",
    "\n",
    "- Decide whether scaling, encoding, or transformations are needed.\n",
    "\n",
    "- Example: Normalize continuous data, one-hot encode categorical data.\n",
    "\n",
    "6. Better Model Selection\n",
    "\n",
    "- EDA insights guide whether the problem suits regression, classification, or clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10137fd0-3b58-4b3c-9162-2ecaa4a92289",
   "metadata": {},
   "source": [
    "# 12. What is correlation? \n",
    "- Correlation is a statistical measure that shows the strength and direction of the relationship between two variables.\n",
    "\n",
    "It tells us whether an increase in one variable is associated with an increase or decrease in another variable.\n",
    "Types of Correlation\n",
    "\n",
    "1. Positive Correlation (+1 → strong positive):\n",
    "\n",
    "- Both variables increase or decrease together.\n",
    "\n",
    "- Example: Height and Weight.\n",
    "\n",
    "2. Negative Correlation (–1 → strong negative):\n",
    "\n",
    "- One variable increases while the other decreases.\n",
    "\n",
    "- Example: Speed of a car and time taken to travel a fixed distance.\n",
    "\n",
    "3. No Correlation (≈ 0):\n",
    "\n",
    "- No relationship between variables.\n",
    "\n",
    "- Example: Shoe size and IQ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4561694-aa85-4c78-bd06-33cca9091e48",
   "metadata": {},
   "source": [
    "# 13. What does negative correlation mean? \n",
    "- Negative correlation means that when one variable increases, the other decreases, and vice versa.\n",
    "\n",
    "- It indicates an inverse relationship between two variables.\n",
    "\n",
    "- Key Points:\n",
    "\n",
    "The correlation coefficient (r) lies between –1 and 0.\n",
    "\n",
    "r = –1 → Perfect negative correlation (variables move exactly opposite).\n",
    "\n",
    "r ≈ 0 → Weak or no correlation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Speed of a car ↑ → Time taken to reach destination ↓\n",
    "\n",
    "Price of a product ↑ → Demand ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bccbc-5b00-428f-95a4-8bff387ee6b3",
   "metadata": {},
   "source": [
    "# 14. How can you find correlation between variables in Python? \n",
    "- In Python, we can find correlation between variables using pandas and numpy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b8cf06b-6a60-4d9c-9f15-08d851ae19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Height    Weight       Age\n",
      "Height  1.000000  1.000000  0.989949\n",
      "Weight  1.000000  1.000000  0.989949\n",
      "Age     0.989949  0.989949  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Using pandas .corr()\n",
    "\n",
    "# Calculates the correlation matrix between all numerical columns.\n",
    "\n",
    "# Default method = Pearson correlation (measures linear relationship) \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'Height': [150, 160, 170, 180, 190],\n",
    "    'Weight': [50, 60, 70, 80, 90],\n",
    "    'Age': [12, 14, 15, 16, 18]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9354fa29-fbeb-4c54-9922-c1fa2a1efebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 2. Using numpy corrcoef() \n",
    "import numpy as np\n",
    "\n",
    "x = [150, 160, 170, 180, 190]\n",
    "y = [50, 60, 70, 80, 90]\n",
    "\n",
    "correlation = np.corrcoef(x, y)\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65e8d4-becd-4828-aed8-9bc11cb22cb0",
   "metadata": {},
   "source": [
    "# 15. What is causation? Explain difference between correlation and causation with an example. \n",
    "- Causation (or causal relationship) means that one variable directly causes a change in another variable.\n",
    "\n",
    "It implies cause and effect, not just association.\n",
    "\n",
    "Example:\n",
    "\n",
    "Smoking → Increases risk of lung cancer\n",
    "difference between correlation and causation :\n",
    "Here, smoking causes lung cancer (causal relationship).\n",
    "1. Correlation:\n",
    "\n",
    "Correlation measures the association or relationship between two variables.\n",
    "\n",
    "It indicates whether the variables move together (positively or negatively), but does not imply one causes the other.\n",
    "\n",
    "Example:\n",
    "\n",
    "Ice cream sales ↑ and temperature ↑\n",
    "\n",
    "These two variables are correlated because they increase together in summer.\n",
    "\n",
    "Important: Buying ice cream does not cause the temperature to rise.\n",
    "\n",
    "2. Causation:\n",
    "\n",
    "Causation indicates a direct cause-and-effect relationship between two variables.\n",
    "\n",
    "One variable directly influences or changes the other.\n",
    "\n",
    "Example:\n",
    "\n",
    "Smoking → Lung cancer\n",
    "\n",
    "Here, smoking is a direct cause of lung cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6129a-2f79-4ae3-8fe0-205e0c1ff1c0",
   "metadata": {},
   "source": [
    "# 16. What is an Optimizer? What are different types of optimizers? Explain each with an example. \n",
    "- In Machine Learning and Deep Learning, an optimizer is an algorithm used to update the model’s parameters (weights and biases) during training in order to minimize the loss function.\n",
    "\n",
    "-The goal of an optimizer is to find the best possible values of parameters that reduce prediction error and improve model performance.\n",
    "Types of Optimizers:\n",
    "\n",
    "1. Batch Gradient Descent (GD)\n",
    "\n",
    "- Iteratively updates model parameters by moving in the direction of the negative gradient of the loss function.\n",
    "- Accurate since it uses the whole dataset.\n",
    "- Very slow for large datasets.\n",
    "- example : Training a linear regression model with the full dataset at once.\n",
    "\n",
    "2. Stochastic Gradient Descent (SGD): \n",
    "- Explanation: Updates weights using one sample at a time.\n",
    "\n",
    "- Pros: Faster than batch gradient descent.\n",
    "\n",
    "- Cons: Noisy updates → may overshoot the minimum.\n",
    "\n",
    "3. Mini-batch Gradient Descent:\n",
    "- Explanation: A balance between Batch and SGD. Uses small batches of data for updates.\n",
    "\n",
    "- Pros: Faster than batch, less noisy than SGD.\n",
    "\n",
    "- Example: Training neural networks in Keras/TensorFlow usually uses mini-batches like size=32 or 64.\n",
    "\n",
    "4. Momentum Optimizer:\n",
    "- Explanation: Adds a “memory” of previous gradients to smooth updates. Prevents oscillations.\n",
    "\n",
    "- Formula:\n",
    "- v=βv−α∇L(w),w=w+v\n",
    "\n",
    "    where \n",
    "    𝛽\n",
    "    β is the momentum factor.\n",
    "\n",
    "- Pros: Speeds up convergence.\n",
    "\n",
    "- Example: Ball rolling down a hill accelerates faster if it has momentum.\n",
    "\n",
    "5. Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "- Explanation: A smarter version of Momentum. Looks ahead before applying updates.\n",
    "\n",
    "- Pros: More accurate convergence than Momentum.\n",
    "\n",
    "- Example: Used in deep learning frameworks for faster training of CNNs.\n",
    "\n",
    "6. Adagrad (Adaptive Gradient Algorithm)\n",
    "\n",
    "- Explanation: Adjusts learning rate individually for each parameter.\n",
    "\n",
    "- Pros: Works well for sparse data (e.g., NLP).\n",
    "\n",
    "- Cons: Learning rate can become very small over time.\n",
    "\n",
    "- Example: Training word embeddings like Word2Vec.\n",
    "\n",
    "7. RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "- Explanation: Solves Adagrad’s problem by controlling the decaying learning rate.\n",
    "\n",
    "- Pros: Great for non-stationary problems (e.g., RNNs).\n",
    "\n",
    "- Example: Commonly used in training recurrent neural networks.\n",
    "\n",
    "8. Adam (Adaptive Moment Estimation)\n",
    "\n",
    "- Explanation: Combines Momentum + RMSprop. Keeps track of both first moment (mean) and second moment  (variance) of gradients.\n",
    "\n",
    "- Pros: Most widely used optimizer in deep learning.\n",
    "\n",
    "9. AdaMax\n",
    "\n",
    "- Explanation: Variant of Adam using infinity norm.\n",
    "\n",
    "- Pros: More stable for high-dimensional data.\n",
    "\n",
    "- Example: Sometimes preferred in NLP models.\n",
    "\n",
    "10. Nadam (Nesterov + Adam)\n",
    "\n",
    "- Explanation: Combines Adam with Nesterov accelerated gradient.\n",
    "\n",
    "- Pros: Often converges faster than Adam.\n",
    "\n",
    "- Example: Training LSTMs for sequence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c58cc-4f35-4f2d-8b61-fac7a454f34b",
   "metadata": {},
   "source": [
    "# 17. What is sklearn.linear_model ? \n",
    "- sklearn.linear_model is a module in the Scikit-learn (sklearn) library of Python. It provides different algorithms for solving problems related to linear models, such as regression and classification. Linear models are those that assume a linear relationship between input features (X) and the target variable (y).\n",
    "- Types of Models in sklearn.linear_model\n",
    "1. Linear Regression\n",
    "\n",
    "- Purpose: Predicts continuous values.\n",
    "\n",
    "- Method: Fits a straight line by minimizing the sum of squared errors.\n",
    "\n",
    "- Example: Predicting house prices based on size and location.\n",
    "\n",
    "2. Logistic Regression\n",
    "\n",
    "- Purpose: Used for binary or multi-class classification.\n",
    "\n",
    "- Method: Uses the sigmoid function to predict probabilities.\n",
    "\n",
    "- Example: Predicting whether an email is spam or not.\n",
    "\n",
    "3. Ridge Regression (L2 Regularization)\n",
    "\n",
    "- Purpose: Prevents overfitting by penalizing large weights.\n",
    "\n",
    "- Method: Adds L2 penalty (sum of squared coefficients) to the loss function.\n",
    "\n",
    "- Example: Useful when input features are highly correlated.\n",
    "\n",
    "4. Lasso Regression (L1 Regularization)\n",
    "\n",
    "- Purpose: Performs both regression and feature selection.\n",
    "\n",
    "- Method: Adds L1 penalty (sum of absolute coefficients) to the loss function, forcing some weights    to zero.\n",
    "\n",
    "- Example: Selecting important predictors in a dataset.\n",
    "\n",
    "5. ElasticNet Regression\n",
    "\n",
    "- Purpose: Combines benefits of both Ridge and Lasso.\n",
    "\n",
    "- Method: Uses both L1 and L2 penalties.\n",
    "\n",
    "- Example: Effective when there are multiple correlated features.\n",
    "\n",
    "6. SGDClassifier and SGDRegressor\n",
    "\n",
    "- Purpose: Train linear models using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "- Method: Updates model parameters one sample or mini-batch at a time.\n",
    "\n",
    "- Example: Works well for large-scale datasets such as text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1a1a8-0052-459b-9615-dd7319b253b5",
   "metadata": {},
   "source": [
    "# 18. What does model.fit() do? What arguments must be given? \n",
    "- The function model.fit() is used to train the model on the training dataset.\n",
    "- It takes input features (X) and target labels (y) and adjusts the internal parameters (like weights in linear models or splits in decision trees) to minimize the error.\n",
    "- After fitting, the model becomes ready to make predictions using model.predict().\n",
    "Arguments of fit()\n",
    "1. Required Arguments\n",
    "\n",
    "    X : Input features (array-like or DataFrame).\n",
    "    \n",
    "    Shape: (n_samples, n_features)\n",
    "    \n",
    "    Example: Height, Weight, Age of people.\n",
    "    \n",
    "    y : Target values (array-like).\n",
    "    \n",
    "    Shape: (n_samples,) for regression or classification.\n",
    "    \n",
    "    Example: House price (regression) or Pass/Fail (classification).\n",
    "\n",
    "2. Optional Arguments (depend on model)\n",
    "\n",
    "    sample_weight : Gives weight to each training example. Useful when some samples are more     important.\n",
    "    \n",
    "    classes : Needed in some classifiers to specify class labels.\n",
    "    \n",
    "    other hyperparameters : Some models may accept additional arguments (like max_iter for Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18194ebe-2809-4f43-87ec-0704e66cfce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of linear regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X = [[1], [2], [3], [4], [5]]     # Input feature\n",
    "y = [2, 4, 6, 8, 10]              # Target\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "model.predict([[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4005e7b-0dfa-4d54-9d62-e9a63a6fa122",
   "metadata": {},
   "source": [
    "# 19. What does model.fit() do? What arguments must be given? \n",
    "- The method model.predict() is used in Machine Learning to generate predictions from a trained model. Once a model has been trained using model.fit(), we can use model.predict() to provide new input data and get the predicted output (labels, values, or probabilities depending on the model type).\n",
    "\n",
    "1. Required Arguments\n",
    "\n",
    "    The main argument for model.predict() is:\n",
    "    \n",
    "    X → The input data on which predictions are to be made.\n",
    "    \n",
    "    It can be a list, NumPy array, Pandas DataFrame, or similar structure.\n",
    "    \n",
    "    Shape should match the features used during training.\n",
    "\n",
    "2. Optional Arguments (depend on library)\n",
    "\n",
    "    batch_size (in deep learning frameworks like Keras/TensorFlow) → Number of samples to process at once.\n",
    "    \n",
    "    verbose → Controls whether progress messages are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63a42065-f342-4d83-a365-c09d8d9f76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10. 12.]\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Training data\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 4, 6, 8])\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict new values\n",
    "X_new = np.array([[5], [6]])\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "print(predictions) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dade1c1-2392-4616-8c27-1c8bb398e87a",
   "metadata": {},
   "source": [
    "# 20. What are continuous and categorical variables? \n",
    "1. Continuous Variables\n",
    "\n",
    "- Definition: Continuous variables are numeric variables that can take any value within a range.\n",
    "\n",
    "- They can be measured and have infinite possible values within an interval.\n",
    "\n",
    "- Usually represent quantities like height, weight, time, salary, temperature.\n",
    "\n",
    "- Can be divided into fractions and decimals.\n",
    "\n",
    "- Example:\n",
    "\n",
    "    Height = 170.5 cm\n",
    "    \n",
    "    Temperature = 36.7 °C\n",
    "    \n",
    "    Salary = ₹50,000.75\n",
    "\n",
    "2. Categorical Variables\n",
    "\n",
    "- Definition: Categorical variables represent data that can be divided into groups or categories.\n",
    "\n",
    "- They are qualitative and not measured but labeled or counted.\n",
    "\n",
    "- Categories may be nominal (no order) or ordinal (have order).\n",
    "\n",
    "- Types of Categorical Variables:\n",
    "\n",
    "- Nominal: Categories without order.\n",
    "\n",
    "- Example: Gender (Male/Female), Blood group (A, B, AB, O).\n",
    "\n",
    "- Ordinal: Categories with logical order.\n",
    "\n",
    "- Example: Education Level (High School < Graduate < Postgraduate).\n",
    "\n",
    "- Example:\n",
    "    \n",
    "    Marital Status = Married/Single/Divorced\n",
    "    \n",
    "    Shirt Size = Small/Medium/Large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba510b-b9ca-41d3-b45b-3dceefc9efb1",
   "metadata": {},
   "source": [
    "# 21. What is feature scaling? How does it help in Machine Learning? \n",
    "- Feature scaling is a data preprocessing technique in Machine Learning where the values of numerical features are transformed to be on a similar scale.\n",
    "\n",
    "- Different features in a dataset may have different units and ranges (e.g., age in years vs. salary  in lakhs).\n",
    "\n",
    "- If not scaled, features with large values may dominate learning algorithms and lead to biased results.\n",
    "\n",
    "- Types of Feature Scaling Methods\n",
    "\n",
    "1. Min-Max Normalization (Rescaling)\n",
    "\n",
    "- Scales values to a fixed range, usually [0,1].\n",
    "\n",
    "2. Standardization (Z-score Normalization)\n",
    "\n",
    "- Centers data around mean = 0 and standard deviation = 1.\n",
    "  \n",
    "3. Robust Scaling\n",
    "\n",
    "- Uses median and interquartile range (IQR).\n",
    "\n",
    "- Less sensitive to outliers.\n",
    "\n",
    "- Helpful in machine learning in various ways:\n",
    "\n",
    "- Feature scaling makes features comparable by adjusting their range.\n",
    "\n",
    "- Helps in faster training, better accuracy, and fair contribution of all features.\n",
    "\n",
    "- Methods: Min-Max, Standardization, Robust Scaling.\n",
    "\n",
    "- Essential for distance-based and gradient-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8e747-4ff3-4702-b6cc-626836bf6e07",
   "metadata": {},
   "source": [
    "# 22. How do we perform scaling in Python? \n",
    "- Scaling in Python is usually done using the sklearn.preprocessing module. The most common techniques are Min-Max Scaling, Standardization, and Robust Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f26e62b-4e34-4a96-9e83-a859e60e6792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaling:\n",
      " [[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [0.75 0.75]\n",
      " [1.   1.  ]]\n",
      "\n",
      "Standardization:\n",
      " [[-1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678]\n",
      " [ 0.          0.        ]\n",
      " [ 0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356]]\n",
      "\n",
      "Robust Scaling:\n",
      " [[-1.  -1. ]\n",
      " [-0.5 -0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.5  0.5]\n",
      " [ 1.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "# steps to perform scaling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "data = np.array([[20, 20000],\n",
    "                 [25, 30000],\n",
    "                 [30, 40000],\n",
    "                 [35, 50000],\n",
    "                 [40, 60000]])\n",
    "\n",
    "# MIN-MAX scaling\n",
    "scaler1 = MinMaxScaler()\n",
    "data_minmax = scaler1.fit_transform(data)\n",
    "print(\"Min-Max Scaling:\\n\", data_minmax)\n",
    "\n",
    "# Standardization (Z-score Normalization) \n",
    "scaler2 = StandardScaler()\n",
    "data_standard = scaler2.fit_transform(data)\n",
    "print(\"\\nStandardization:\\n\", data_standard)\n",
    "\n",
    "# Robust Scaling \n",
    "scaler3 = RobustScaler()\n",
    "data_robust = scaler3.fit_transform(data)\n",
    "print(\"\\nRobust Scaling:\\n\", data_robust)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3b83c-3f47-44ff-8471-defa8fda6500",
   "metadata": {},
   "source": [
    "# 23. What is sklearn.preprocessing?  \n",
    "- sklearn.preprocessing is a module in Scikit-Learn (sklearn) that provides tools and functions to prepare raw data for Machine Learning models.Since most ML algorithms work better when input data is in proper scale or encoded form, preprocessing ensures that features are normalized,standardized or encoded before training.\n",
    "- Key Functions in sklearn.preprocessing\n",
    "\n",
    "1. Scaling and Normalization\n",
    "\n",
    "- StandardScaler → Standardizes features (mean = 0, variance = 1).\n",
    "\n",
    "- MinMaxScaler → Scales features to a given range, usually [0,1].\n",
    "\n",
    "- Normalizer → Scales each sample individually to unit norm (useful in text classification or clustering).\n",
    "\n",
    "2. Encoding Categorical Data\n",
    "\n",
    "- LabelEncoder → Converts categorical labels (e.g., \"Male\", \"Female\") into numbers.\n",
    "\n",
    "- OneHotEncoder → Converts categorical variables into binary columns.\n",
    "\n",
    "- OrdinalEncoder → Encodes categorical features as integers, respecting order.\n",
    "\n",
    "3. Generating Polynomial Features\n",
    "- PolynomialFeatures → Creates new features by combining existing ones (useful in polynomial regression).\n",
    "\n",
    "4. Imputation (Handling Missing Data)\n",
    "\n",
    "- SimpleImputer → Fills missing values with mean, median, mode, or a constant.\n",
    "\n",
    "5. Discretization (Converting Continuous → Categorical)\n",
    "- KBinsDiscretizer → Divides continuous values into bins (e.g., Age groups)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5bddf-2871-4d3a-87e3-50ac722630df",
   "metadata": {},
   "source": [
    "# 24. How do we split data for model fitting (training and testing) in Python? \n",
    "- In Machine Learning, splitting data is the process of dividing the dataset into:\n",
    "\n",
    "- Training set → Used to train the model.\n",
    "\n",
    "- Testing set → Used to evaluate the model’s performance on unseen data.\n",
    "\n",
    "- This helps prevent overfitting and ensures the model can generalize to new data.\n",
    "\n",
    "- Common split ratios: 70:30, 80:20, 75:25 (Training:Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0959c36e-fdbe-4b9d-9141-fb1db64ea878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " [[5]\n",
      " [3]\n",
      " [1]\n",
      " [4]]\n",
      "X_test:\n",
      " [[2]]\n",
      "y_train:\n",
      " [50 30 10 40]\n",
      "y_test:\n",
      " [20]\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Sample features and labels\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([10, 20, 30, 40, 50])\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train:\\n\", X_train)\n",
    "print(\"X_test:\\n\", X_test)\n",
    "print(\"y_train:\\n\", y_train)\n",
    "print(\"y_test:\\n\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4cf69-2b47-43cd-a955-fd7edb243724",
   "metadata": {},
   "source": [
    "# 25. Explain data encoding? \n",
    "- Data encoding is a preprocessing technique used to convert categorical data into numerical form so that Machine Learning algorithms can understand and process it.\n",
    "\n",
    "- Most ML algorithms work only with numbers, not text or categories.\n",
    "\n",
    "- Encoding transforms labels, categories, or text features into numbers or vectors.\n",
    "\n",
    "- Types of Data Encoding\n",
    "\n",
    "  \n",
    "(a) Label Encoding\n",
    "\n",
    "    Converts each category to a unique integer.\n",
    "    \n",
    "    Good for ordinal categorical variables (where order matters).\n",
    "\n",
    "(b) One-Hot Encoding\n",
    "\n",
    "    Converts each category into a binary vector (0 or 1).\n",
    "    \n",
    "    Useful for nominal categorical variables (no order).\n",
    "\n",
    "(c)` Binary Encoding: Combines Label Encoding and One-Hot for high-cardinality features.\n",
    "\n",
    "(d) Target Encoding: Replaces categories with mean of the target variable.\n",
    "\n",
    "(e) Frequency Encoding: Uses frequency of each category as value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96f79e-b827-4a6c-b586-e58cbccce2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
